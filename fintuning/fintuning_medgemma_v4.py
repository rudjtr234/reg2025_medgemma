

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MedGemma ëª¨ë¸ QLoRA ê¸°ë°˜ Fine-Tuning ìŠ¤í¬ë¦½íŠ¸ (v0.2, ë©€í‹° ì´ë¯¸ì§€/ìƒ˜í”Œ)

ê°œìš”:
-----
ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” MedGemma ëª¨ë¸ì— ëŒ€í•´ 4-bit QLoRA ë°©ì‹ì„ ì ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
ë‹¨ì¼ ì´ë¯¸ì§€ë¿ ì•„ë‹ˆë¼ ë³µìˆ˜ ì´ë¯¸ì§€ ì…ë ¥ì„ ì§€ì›í•˜ë©°, ë‹¤ì¤‘ GPU í™˜ê²½ì—ì„œì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ ê°œì„ ì„ ì¤‘ì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.

ì£¼ìš” ê°œì„  ì‚¬í•­:
---------------
1. ì´ë¯¸ì§€ ë¡œë”© ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ë¡œë”© (ì˜¤ë¥˜ ì‹œ ê²€ì • ì´ë¯¸ì§€ ëŒ€ì²´)
2. BLEU ë° METEOR ê¸°ë°˜ í‰ê°€ ì§€ì› (ë¶„ì‚° ì‹¤í–‰ ì‹œ CLIP Score ì œì™¸)
3. dataloader_num_workers = 0 ì„¤ì •ìœ¼ë¡œ ë‹¤ì¤‘ GPU í™˜ê²½ì—ì„œì˜ ì•ˆì •ì„± ê°•í™”
4. ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œì˜ JSON ì…ë ¥ ë° ì´ë¯¸ì§€ ì…ë ¥ ìƒíƒœ í™•ì¸ ì¶”ê°€
5. JSON ë‚´ ë¶ˆí•„ìš” ê³µë°± ë° ì˜ëª»ëœ í…ìŠ¤íŠ¸ ì œê±°
6. í•™ìŠµìš©(train)ê³¼ í‰ê°€ìš©(eval) ë°ì´í„°ì…‹ ë¶„ë¦¬
7. accelerate ê¸°ë°˜ ë©€í‹° GPU í•™ìŠµ í™˜ê²½ìœ¼ë¡œ ì „í™˜ (ë‹¨ì¼ GPU ì½”ë“œ â†’ DDP/Acceleratorë¡œ í†µí•©)
8. ì‚¬ìš© ì˜ˆì‹œ í¬í•¨ (ì´ 8ê°œ GPU: NVIDIA RTX 6000 Ada ì‚¬ìš©)


"""

import argparse
import datetime
import os
import random
import mimetypes
from typing import Any, Dict, List, Optional, Sequence, Tuple
from peft import get_peft_model, LoraConfig
import torch
from PIL import Image
from accelerate import Accelerator
from datasets import load_dataset
from evaluate import load as evload
from transformers import (
    AutoModelForImageTextToText,
    AutoProcessor,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTConfig, SFTTrainer

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fineâ€‘tune MedGemma with QLoRA")

    parser.add_argument("--model_path", default="medgemma-4b-it", help="Path to the pretrained MedGemma model.")
    parser.add_argument("--train_json", required=True, help="JSON file containing training data with image paths and messages.")
    parser.add_argument("--epochs", type=int, default=3, help="Number of training epochs.")
    parser.add_argument("--lr", type=float, default=3e-5, help="Learning rate.")
    parser.add_argument("--rank", type=int, default=16, help="LoRA rank (r).")
    parser.add_argument("--batch_size", type=int, default=16, help="Per-device batch size.")
    parser.add_argument("--grad_accum", type=int, default=2, help="Gradient accumulation steps.")
    parser.add_argument("--save_steps", type=int, default=2000, help="Steps between saving checkpoints.")
    parser.add_argument("--eval_steps", type=int, default=500, help="Steps between evaluations.")
    parser.add_argument("--logging_steps", type=int, default=50, help="Steps between logging loss.")
    parser.add_argument("--num_workers", type=int, default=8, help="Number of data loader workers.")
    parser.add_argument("--image_size", type=int, default=896, help="Tile image size.")
    parser.add_argument("--no_vis", action="store_true", help="Skip visualizations and checks.")
    parser.add_argument("--output_dir", type=str, default=None, help="Where to save final model")

    return parser.parse_args()

def print_trainable_parameters_custom(model):
    trainable = 0
    total = 0
    for name, param in model.named_parameters():
        total += param.numel()
        if param.requires_grad:
            trainable += param.numel()
    print(f"ğŸ”§ Trainable params: {trainable} | Total params: {total} | Trainable%: {100 * trainable / total:.4f}%")

def prepare_model_and_processor(
    model_path: str,
    lora_rank: int,
    accelerator: Accelerator,
) -> Tuple[Any, Any, List[str]]:
    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_storage=torch.float32,
    )

    with accelerator.main_process_first():
        model = AutoModelForImageTextToText.from_pretrained(
            model_path,
            quantization_config=bnb_cfg,
            torch_dtype=torch.float16,
            attn_implementation="eager",
        )
        model = prepare_model_for_kbit_training(model)

    model.gradient_checkpointing_disable()

    # Vision Tower gradient checkpointing
    vt = model.vision_tower
    vision_layers = (
        vt.encoder.layers
        if hasattr(vt, "encoder")
        else vt.vision_model.encoder.layers
    )
    for blk in vision_layers:
        blk.gradient_checkpointing = True

# LoRA ì ìš© ëŒ€ìƒ ë¶„ë¦¬
    vision_linear_modules = [
        name for name, module in model.named_modules()
        if isinstance(module, torch.nn.Linear) and any(k in name.lower() for k in ["vision", "image", "vit", "visual"])
        ] 

    language_linear_modules = [
        name for name, module in model.named_modules()
        if isinstance(module, torch.nn.Linear) and not any(k in name.lower() for k in ["vision", "image", "vit", "visual"])
        ]


    targets = vision_linear_modules + language_linear_modules

    print("ğŸ¯ LoRA will be applied to the following Linear modules:")
    print("\nğŸ§  Vision ê´€ë ¨ ë ˆì´ì–´:")
    if vision_linear_modules:
        for name in vision_linear_modules:
            print(f"  - {name}")
    else:
        print("  âŒ ì—†ìŒ")

    print("\nğŸ§  Language ê´€ë ¨ ë ˆì´ì–´:")
    for name in language_linear_modules:
        print(f"  - {name}")

    lora_cfg = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_rank * 4,
        lora_dropout=0.05,
        target_modules=targets,
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_cfg)

    print("ğŸ§© Trainable parameters summary after applying LoRA:")
    if hasattr(model, "print_trainable_parameters"):
        model.print_trainable_parameters()
    else:
        print_trainable_parameters_custom(model)

    processor = AutoProcessor.from_pretrained(model_path)
    return model, processor, targets



def load_and_split_dataset(
    json_path: str,
    accelerator: Accelerator,
) -> Tuple[Any, Any, Any]:
    def is_valid(example: Dict[str, Any]) -> bool:
        return (
            example.get("image")
            and os.path.exists(example["image"])
            and example.get("messages")
        )

    with accelerator.main_process_first():
        ds_all = load_dataset("json", data_files={"train": json_path})["train"].filter(is_valid)

    indices = list(range(len(ds_all)))
    random.seed(42)
    random.shuffle(indices)
    cut = int(len(indices) * 0.9)
    train_idx = indices[:cut]
    val_idx = indices[cut:] or indices[-1:]

    ds_train = ds_all.select(train_idx)
    ds_val = ds_all.select(val_idx)

    if accelerator.is_main_process:
        print(f"Total samples: {len(ds_all):,}")
        print(f"Train: {len(ds_train):,} | Val: {len(ds_val):,}")

    return ds_train, ds_val, ds_all

def load_image(path: str) -> Any:
    """
    Safely load an image or tensor from disk. If loading fails, return
    a black placeholder image instead of raising an exception.
    """
    mime, _ = mimetypes.guess_type(path)
    try:
        if mime and mime.startswith("image"):
            return Image.open(path).convert("RGB")
        return torch.load(path, map_location="cpu")
    except Exception as e:
        print(f"[WARN] Failed to load '{path}': {e}. Using fallback image.")
        return Image.new("RGB", (224, 224), color="black")

def collate_fn(
    batch: Sequence[Dict[str, Any]],
    processor: Any,
    ds_all: Any,
) -> Dict[str, Any]:
    def is_valid(example: Dict[str, Any]) -> bool:
        return (
            example.get("image")
            and os.path.exists(example["image"])
            and example.get("messages")
        )

    batch = [ex for ex in batch if is_valid(ex)] or [ds_all[0]]
    texts: List[str] = []
    images: List[List[Any]] = []
    for ex in batch:
        texts.append(
            processor.apply_chat_template(
                ex["messages"],
                add_generation_prompt=False,
                tokenize=False,
            ).strip()
        )
        images.append([load_image(ex["image"])])

    out = processor(text=texts, images=images, padding=True, return_tensors="pt")
    labels = out["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100

    start_id = processor.tokenizer.convert_tokens_to_ids("<start_of_image>")
    soft_id = processor.tokenizer.convert_tokens_to_ids("<image_soft_token>")
    for i, ids in enumerate(out["input_ids"]):
        positions = (ids == start_id).nonzero(as_tuple=True)[0]
        for p in positions:
            q = p + 1
            while q < ids.size(0) and soft_id <= ids[q] < soft_id + 256:
                q += 1
            labels[i, p:q] = -100

    out["labels"] = labels
    return out

def load_metrics() -> Tuple[Any, Optional[Any]]:
    """
    Load evaluation metrics. Returns BLEU and METEOR (if available).
    CLIP score is omitted to avoid image loading in metrics.
    """
    bleu = evload("bleu")
    try:
        import nltk  # type: ignore
        nltk.download("wordnet", quiet=True)
        meteor = evload("meteor")
    except Exception:
        meteor = None
    return bleu, meteor

def compute_metrics_factory(
    processor: Any,
    bleu_metric: Any,
    meteor_metric: Optional[Any],
) -> Any:
    def compute_metrics(eval_pred: Tuple[torch.Tensor, torch.Tensor]) -> Dict[str, float]:
        preds, labels = eval_pred
        preds_txt = processor.batch_decode(preds, skip_special_tokens=True)
        labels_txt = processor.batch_decode(labels, skip_special_tokens=True)
        results = {
            "bleu": bleu_metric.compute(
                predictions=preds_txt,
                references=[[t] for t in labels_txt],
            )["bleu"]
        }
        if meteor_metric is not None:
            results["meteor"] = meteor_metric.compute(
                predictions=preds_txt,
                references=[[t] for t in labels_txt],
            )["meteor"]
        return results
    return compute_metrics


from transformers import TrainerCallback, TrainerState, TrainerControl, TrainingArguments

class PrintLossCallback(TrainerCallback):
    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            print(f"[Step {state.global_step}] Training loss: {logs['loss']:.4f}")

class PrintEvalCallback(TrainerCallback):
    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict, **kwargs):
        print(f"\n[âœ… Evaluation @ Step {state.global_step}]")
        for k, v in metrics.items():
            if isinstance(v, float):
                print(f"  {k}: {v:.4f}")
            else:
                print(f"  {k}: {v}")



import datetime
import os
from transformers import TrainerCallback
from torch.utils.tensorboard import SummaryWriter
from nltk.translate.bleu_score import sentence_bleu
import torch

# âœ… 2. on_epoch_end ìˆ˜ì •
class EvalOnEpochEndCallback(TrainerCallback):
    def __init__(self, val_dataset, processor, writer=None, max_samples=30):
        self.val_dataset = val_dataset
        self.processor = processor
        self.writer = writer or SummaryWriter()
        self.max_samples = max_samples

    def on_epoch_end(self, args, state, control, model=None, **kwargs):
        print(f"\n[EVAL] Epoch {state.epoch:.2f} - Generating BLEU score...")
        model.eval()
        scores = []

        for i, sample in enumerate(self.val_dataset):
            if i >= self.max_samples:
                break

            image = sample["image"]

            # âœ… flatten ì ìš©
            try:
                flattened = flatten_messages(sample["messages"])
                if not flattened:
                    print(f"[WARN] ë©”ì‹œì§€ flatten ì‹¤íŒ¨ â†’ ê±´ë„ˆëœ€: {sample}")
                    continue

                gt_text = self.processor.apply_chat_template(
                    flattened,
                    add_generation_prompt=False,
                    tokenize=False
                ).strip()

                if not gt_text:
                    print(f"[WARN] ë¹ˆ gt_text ìƒì„± â†’ ê±´ë„ˆëœ€: {sample}")
                    continue

            except Exception as e:
                print(f"[ERROR] ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e} â†’ ê±´ë„ˆëœ€")
                continue

            # âœ… text ì¸ì í¬í•¨í•˜ì—¬ processor í˜¸ì¶œ
            try:
                inputs = self.processor(
                    text=[gt_text],
                    images=[load_image(image)],
                    return_tensors="pt"
                ).to(model.device)

                with torch.no_grad():
                    generated_ids = model.generate(**inputs, max_new_tokens=128)

                pred_text = self.processor.tokenizer.decode(
                    generated_ids[0], skip_special_tokens=True
                )

                bleu_score = sentence_bleu([gt_text.split()], pred_text.split())
                scores.append(bleu_score)

            except Exception as e:
                print(f"[ERROR] ì¶”ë¡  ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e} â†’ ê±´ë„ˆëœ€")
                continue

        avg_bleu = sum(scores) / len(scores) if scores else 0.0
        print(f"[EVAL] Epoch {state.epoch:.2f} - BLEU score: {avg_bleu:.4f}")
        self.writer.add_scalar("eval/bleu", avg_bleu, int(state.epoch))




from trl import SFTTrainer, SFTConfig
from accelerate import Accelerator

def main() -> None:
    args = parse_args()
    accelerator = Accelerator(mixed_precision="fp16")

    # 1. ëª¨ë¸ + LoRA ì„¤ì •
    model, processor, used_lora_layers = prepare_model_and_processor(
        model_path=args.model_path,
        lora_rank=args.rank,
        accelerator=accelerator,
    )

    # ğŸ” LoRA íŒŒë¼ë¯¸í„° í™•ì¸
    print("ğŸ§© Trainable parameters summary after applying LoRA:")
    for layer in used_lora_layers:
        print(f"  - {layer}")

    # âœ… ViT ê´€ë ¨ ë ˆì´ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸
    vision_keywords = ["vision", "vit", "image", "visual"]
    has_vision_layer = any(any(k in layer_name.lower() for k in vision_keywords) for layer_name in used_lora_layers)

    if has_vision_layer:
        print("âœ… LoRAê°€ Vision ê´€ë ¨ ë ˆì´ì–´ì—ë„ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ Vision ê´€ë ¨ ë ˆì´ì–´ì—ëŠ” LoRAê°€ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 2. ë°ì´í„°ì…‹ ë¡œë”©
    ds_train, ds_val, ds_all = load_and_split_dataset(
        json_path=args.train_json,
        accelerator=accelerator,
    )

    # 3. ì‹œê°í™” ë° í…ìŠ¤íŠ¸ ê²€ì¦
    if not args.no_vis and accelerator.is_main_process:
        unique_labels = set()
        for i in range(min(1000, len(ds_all))):
            for message in ds_all[i]["messages"]:
                if message.get("role") == "assistant":
                    for content in message.get("content", []):
                        text = content.get("text")
                        if text:
                            unique_labels.add(text.strip())
        label_count = len(unique_labels)
        print(f"ğŸ“Š Unique labels: {label_count}")
        if label_count < 2:
            print("âš ï¸ Warning: low label diversity detected.")

        sample_batch = [ds_all[i] for i in range(min(4, len(ds_all)))]
        texts = [
            processor.apply_chat_template(
                s["messages"],
                add_generation_prompt=False,
                tokenize=False,
            ).strip()
            for s in sample_batch
        ]
        images = [[load_image(s["image"])] for s in sample_batch]
        encoded = processor(
            text=texts,
            images=images,
            padding=True,
            return_tensors="pt",
        )
        labels = encoded["input_ids"].clone()
        labels[labels == processor.tokenizer.pad_token_id] = -100
        valid_tokens = (labels != -100).sum().item()
        if valid_tokens == 0:
            raise ValueError("All labels were masked to -100. Check your dataset and processor.")
        print("âœ… Processor masking test passed.")

    # â›” í•™ìŠµ ë¡œì§ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
    timestamp = datetime.datetime.now().strftime("%m%d_%H%M")
    output_dir = os.path.join(
        os.path.dirname(args.train_json),
        f"finetuned_medgemma_qlora_{timestamp}",
    )

    trainer = SFTTrainer(
        model=model,
        args=SFTConfig(
            output_dir=output_dir,
            num_train_epochs=args.epochs,                                 # âœ… epochs ë°˜ì˜
            per_device_train_batch_size=args.batch_size,                  # âœ… CLI ì¸ì ë°˜ì˜
            gradient_accumulation_steps=args.grad_accum,                  # âœ…
            learning_rate=args.lr,                                        # âœ…
            lr_scheduler_type="cosine",
            warmup_ratio=0.05,
            fp16=True,
            logging_steps=args.logging_steps,                             # âœ…
            save_strategy="steps",
            save_steps=args.save_steps,                                   # âœ…
            eval_steps=args.eval_steps,                                   # âœ…
            dataloader_num_workers=args.num_workers,                      # âœ…
            ddp_find_unused_parameters=True,
            max_grad_norm=1.0,
            label_names=["labels"],
            report_to=["tensorboard"],
        ),
        train_dataset=ds_train,
        eval_dataset=ds_val,
        compute_metrics=None,  # ì½œë°±ì—ì„œ ì§ì ‘ í‰ê°€
        data_collator=lambda batch: collate_fn(batch, processor, ds_all),
        callbacks=[
            PrintLossCallback(),
            PrintEvalCallback(),
            EvalOnEpochEndCallback(ds_val, processor),
        ],
    )

    if accelerator.is_main_process:
        print("Starting training...")
    trainer.train()
  
    final_out_dir = args.output_dir or os.path.join(
        os.path.dirname(args.train_json),
        f"medgemma_final_v.0.1.3",
        )   

    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        model.save_pretrained(final_out_dir, safe_serialization=True)
        print(f"Adapter saved to {final_out_dir} as safetensors")

if __name__ == "__main__":
    main()


